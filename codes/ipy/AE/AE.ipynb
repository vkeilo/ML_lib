{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Lambda\n",
    "from keras.layers import Embedding\n",
    "from keras.datasets import imdb\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Convolution1D, Dense, Dropout, Flatten, MaxPooling1D,UpSampling1D\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "import h5py\n",
    "from keras import callbacks\n",
    "from keras.layers import LSTM, GRU, SimpleRNN\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
    "from sklearn.metrics import (precision_score, recall_score,f1_score, accuracy_score,mean_squared_error,mean_absolute_error)\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 获取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-83231f068ae1>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\IDS-1\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\IDS-1\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\IDS-1\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\IDS-1\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\IDS-1\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = mnist.test.images # 图片10000\n",
    "test_y = mnist.test.labels # 标签\n",
    "train_x=mnist.train.images #55000\n",
    "train_y=mnist.train.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = train_data.iloc[:,0:weidu_x_ori]\n",
    "#Y = train_data.iloc[:,weidu_x_ori:]\n",
    "#T = test_data.iloc[:,0:weidu_x_ori]\n",
    "#C = test_data.iloc[:,weidu_x_ori:]\n",
    "\n",
    "X=train_x\n",
    "Y=train_y\n",
    "T=test_x\n",
    "C=test_y\n",
    "#scaler = Normalizer().fit(X)\n",
    "#trainX = scaler.transform(X)\n",
    "\n",
    "#scaler = Normalizer().fit(T)\n",
    "#testT = scaler.transform(T)\n",
    "\n",
    "trainX=train_x\n",
    "testT=test_x\n",
    "y_train = train_y\n",
    "y_test = test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "weidu_x_ori=28*28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\IDS-1\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\IDS-1\\lib\\site-packages\\ipykernel_launcher.py:23: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
      "D:\\Anaconda\\envs\\IDS-1\\lib\\site-packages\\ipykernel_launcher.py:27: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
      "D:\\Anaconda\\envs\\IDS-1\\lib\\site-packages\\ipykernel_launcher.py:39: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "import keras\n",
    "# this is the size of our encoded representations\n",
    "mid_num=100\n",
    "input_size=weidu_x_ori\n",
    "encoding_dim = mid_num  # \n",
    "#编码大小24\n",
    "\n",
    "# this is our input placeholder\n",
    "input_img = Input(shape=(input_size,))#自编码器输入shape\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "\n",
    "#编码部分\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_img)\n",
    "\n",
    "#解码部分，损失部分信息\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded = Dense(input_size ,activation='sigmoid')(encoded)\n",
    "#decoded = Dense(input_size ,activation='sigmoid',kernel_regularizer=keras.regularizers.l1(0.01))(encoded)#sigmoid,relu,tanh\n",
    "\n",
    "#建立自编码解码器模型\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(input=input_img, output=decoded)\n",
    "\n",
    "#建立自编码器模型\n",
    "# this model maps an input to its encoded representation\n",
    "encoder = Model(input=input_img, output=encoded)\n",
    "\n",
    "#占位符等待输入——编码输入 64维\n",
    "# create a placeholder for an encoded (32-dimensional) input\n",
    "encoded_input = Input(shape=(encoding_dim,))\n",
    "\n",
    "#自编码器最后一层\n",
    "# retrieve the last layer of the autoencoder model\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "\n",
    "#建立解码器模型\n",
    "# create the decoder model\n",
    "decoder = Model(input=encoded_input, output=decoder_layer(encoded_input))\n",
    "\n",
    "#总模型优化目标\n",
    "autoencoder.compile(optimizer='adadelta', loss='categorical_crossentropy')#mse/binary_crossentropy/categorical_crossentropy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\IDS-1\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\IDS-1\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\IDS-1\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 55000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "55000/55000 [==============================] - 1s 16us/step - loss: 624.9990 - val_loss: 599.0230\n",
      "Epoch 2/50\n",
      "55000/55000 [==============================] - 1s 13us/step - loss: 581.5910 - val_loss: 579.5895\n",
      "Epoch 3/50\n",
      "55000/55000 [==============================] - 1s 14us/step - loss: 565.2680 - val_loss: 566.1832\n",
      "Epoch 4/50\n",
      "55000/55000 [==============================] - 1s 14us/step - loss: 555.2472 - val_loss: 559.9542\n",
      "Epoch 5/50\n",
      "55000/55000 [==============================] - 1s 14us/step - loss: 548.1380 - val_loss: 551.7092\n",
      "Epoch 6/50\n",
      "55000/55000 [==============================] - 1s 13us/step - loss: 542.9272 - val_loss: 548.2842\n",
      "Epoch 7/50\n",
      "55000/55000 [==============================] - 1s 14us/step - loss: 539.3709 - val_loss: 544.9507\n",
      "Epoch 8/50\n",
      "55000/55000 [==============================] - 1s 14us/step - loss: 536.3645 - val_loss: 542.3592\n",
      "Epoch 9/50\n",
      "55000/55000 [==============================] - 1s 13us/step - loss: 533.8169 - val_loss: 540.0749\n",
      "Epoch 10/50\n",
      "55000/55000 [==============================] - 1s 13us/step - loss: 531.7251 - val_loss: 537.4900\n",
      "Epoch 11/50\n",
      "55000/55000 [==============================] - 1s 14us/step - loss: 529.9416 - val_loss: 536.7612\n",
      "Epoch 12/50\n",
      "55000/55000 [==============================] - 1s 13us/step - loss: 528.5264 - val_loss: 535.1560\n",
      "Epoch 13/50\n",
      "55000/55000 [==============================] - 1s 13us/step - loss: 527.0891 - val_loss: 534.4302\n",
      "Epoch 14/50\n",
      "55000/55000 [==============================] - 1s 13us/step - loss: 526.2210 - val_loss: 532.7793\n",
      "Epoch 15/50\n",
      "55000/55000 [==============================] - 1s 14us/step - loss: 525.0381 - val_loss: 532.2332\n",
      "Epoch 16/50\n",
      "55000/55000 [==============================] - 1s 14us/step - loss: 523.7640 - val_loss: 530.8404\n",
      "Epoch 17/50\n",
      "55000/55000 [==============================] - 1s 14us/step - loss: 523.0761 - val_loss: 530.2749\n",
      "Epoch 18/50\n",
      "55000/55000 [==============================] - 1s 13us/step - loss: 522.2563 - val_loss: 529.7494\n",
      "Epoch 19/50\n",
      "55000/55000 [==============================] - 1s 14us/step - loss: 521.5783 - val_loss: 528.6670\n",
      "Epoch 20/50\n",
      "55000/55000 [==============================] - 1s 14us/step - loss: 520.9924 - val_loss: 528.2975\n",
      "Epoch 21/50\n",
      "55000/55000 [==============================] - 1s 14us/step - loss: 520.5292 - val_loss: 527.5881\n",
      "Epoch 22/50\n",
      "55000/55000 [==============================] - 1s 13us/step - loss: 519.8839 - val_loss: 527.7749\n",
      "Epoch 23/50\n",
      "55000/55000 [==============================] - 1s 15us/step - loss: 519.2273 - val_loss: 526.2138\n",
      "Epoch 24/50\n",
      "55000/55000 [==============================] - 1s 14us/step - loss: 518.6409 - val_loss: 526.6523\n",
      "Epoch 25/50\n",
      "55000/55000 [==============================] - 1s 14us/step - loss: 518.7244 - val_loss: 525.6392\n",
      "Epoch 26/50\n",
      "55000/55000 [==============================] - 1s 14us/step - loss: 518.1137 - val_loss: 525.7253\n",
      "Epoch 27/50\n",
      "55000/55000 [==============================] - 1s 15us/step - loss: 517.5472 - val_loss: 524.8150\n",
      "Epoch 28/50\n",
      "55000/55000 [==============================] - 1s 14us/step - loss: 517.5613 - val_loss: 525.5348\n",
      "Epoch 29/50\n",
      "55000/55000 [==============================] - 1s 13us/step - loss: 516.9678 - val_loss: 524.3296\n",
      "Epoch 30/50\n",
      "55000/55000 [==============================] - 1s 14us/step - loss: 516.7011 - val_loss: 524.3465\n",
      "Epoch 31/50\n",
      "55000/55000 [==============================] - 1s 14us/step - loss: 516.3631 - val_loss: 523.4414\n",
      "Epoch 32/50\n",
      "55000/55000 [==============================] - 1s 14us/step - loss: 515.9409 - val_loss: 523.8989\n",
      "Epoch 33/50\n",
      "55000/55000 [==============================] - 1s 14us/step - loss: 515.9871 - val_loss: 523.3993\n",
      "Epoch 34/50\n",
      "55000/55000 [==============================] - 1s 13us/step - loss: 515.6127 - val_loss: 523.3853\n",
      "Epoch 35/50\n",
      "55000/55000 [==============================] - 1s 14us/step - loss: 515.3592 - val_loss: 522.5304\n",
      "Epoch 36/50\n",
      "55000/55000 [==============================] - 1s 13us/step - loss: 515.0489 - val_loss: 524.0298\n",
      "Epoch 37/50\n",
      "55000/55000 [==============================] - 1s 14us/step - loss: 515.1866 - val_loss: 522.1525\n",
      "Epoch 38/50\n",
      "55000/55000 [==============================] - 1s 14us/step - loss: 514.5983 - val_loss: 522.1514\n",
      "Epoch 39/50\n",
      "55000/55000 [==============================] - 1s 14us/step - loss: 514.5078 - val_loss: 521.8352\n",
      "Epoch 40/50\n",
      "55000/55000 [==============================] - 1s 14us/step - loss: 514.3069 - val_loss: 522.1288\n",
      "Epoch 41/50\n",
      "55000/55000 [==============================] - 1s 14us/step - loss: 514.1818 - val_loss: 521.4462\n",
      "Epoch 42/50\n",
      "55000/55000 [==============================] - 1s 14us/step - loss: 513.9633 - val_loss: 522.0352\n",
      "Epoch 43/50\n",
      "55000/55000 [==============================] - 1s 14us/step - loss: 513.8625 - val_loss: 521.2130\n",
      "Epoch 44/50\n",
      "55000/55000 [==============================] - 1s 14us/step - loss: 513.6915 - val_loss: 521.4575\n",
      "Epoch 45/50\n",
      "55000/55000 [==============================] - 1s 14us/step - loss: 513.5333 - val_loss: 521.0311\n",
      "Epoch 46/50\n",
      "55000/55000 [==============================] - 1s 14us/step - loss: 513.3988 - val_loss: 521.0531\n",
      "Epoch 47/50\n",
      "55000/55000 [==============================] - 1s 13us/step - loss: 513.1961 - val_loss: 520.5696\n",
      "Epoch 48/50\n",
      "55000/55000 [==============================] - 1s 14us/step - loss: 512.9218 - val_loss: 521.3117\n",
      "Epoch 49/50\n",
      "55000/55000 [==============================] - 1s 13us/step - loss: 512.9366 - val_loss: 520.6605\n",
      "Epoch 50/50\n",
      "55000/55000 [==============================] - 1s 14us/step - loss: 512.9094 - val_loss: 520.3417\n",
      "10000/10000 [==============================] - 0s 35us/step\n",
      "layer-1损失为：609.0986397460938\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#开始学习  50*256\n",
    "autoencoder.fit(trainX, trainX,\n",
    "                nb_epoch=50,\n",
    "                batch_size=3000,\n",
    "                shuffle=True,\n",
    "                #device_count={\"CPU\":12},\n",
    "                validation_data=(testT, testT))\n",
    "loss_layer1=autoencoder.evaluate(testT, autoencoder.predict(testT), verbose = 1)#, verbose = 0代表计算过程不显示\n",
    "print(\"layer-1损失为：\"+str(loss_layer1)+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_train_data = pd.DataFrame(encoder.predict(trainX))\n",
    "encoded_test_data = pd.DataFrame(encoder.predict(testT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0000000e+00, 1.1920929e-07, 0.0000000e+00, ..., 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00],\n",
       "       [0.0000000e+00, 2.9802322e-08, 0.0000000e+00, ..., 0.0000000e+00,\n",
       "        5.9604645e-08, 0.0000000e+00],\n",
       "       [8.9406967e-08, 8.9406967e-08, 1.1920929e-07, ..., 5.9604645e-08,\n",
       "        8.9406967e-08, 5.9604645e-08],\n",
       "       ...,\n",
       "       [5.3644180e-07, 1.1920929e-06, 3.8743019e-07, ..., 1.4901161e-07,\n",
       "        7.1525574e-07, 1.1920929e-07],\n",
       "       [5.0961971e-06, 1.1920929e-07, 1.2218952e-06, ..., 6.1690807e-06,\n",
       "        7.1525574e-07, 1.3709068e-06],\n",
       "       [2.7120113e-06, 3.3974648e-06, 9.5367432e-07, ..., 6.5565109e-07,\n",
       "        1.5497208e-06, 2.2053719e-06]], dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.predict(trainX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "position=1600\n",
    "ori=trainX[position].reshape(28,28)\n",
    "#mid=encoder.predict(trainX[position].reshape(1,28*28)).reshape(7,7)\n",
    "en_decoded=autoencoder.predict(trainX[position].reshape(1,28*28)).reshape(28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#autoencoder.predict(trainX[200].reshape(1,28*28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x14fcd509dd8>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(ori,'bone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x14fcd5697f0>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(en_decoded,'bone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "weidu_encoded=len(encoded_train_data.iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.398752</td>\n",
       "      <td>6.278047</td>\n",
       "      <td>3.580938</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.484455</td>\n",
       "      <td>6.600603</td>\n",
       "      <td>1.099844</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.563326</td>\n",
       "      <td>2.124260</td>\n",
       "      <td>5.068234</td>\n",
       "      <td>5.630467</td>\n",
       "      <td>5.340586</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.196835</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.042018</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.725769</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.281141</td>\n",
       "      <td>3.571757</td>\n",
       "      <td>3.438337</td>\n",
       "      <td>2.027230</td>\n",
       "      <td>2.803521</td>\n",
       "      <td>0.249094</td>\n",
       "      <td>2.896202</td>\n",
       "      <td>1.898718</td>\n",
       "      <td>...</td>\n",
       "      <td>6.233778</td>\n",
       "      <td>3.301990</td>\n",
       "      <td>2.414010</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.431331</td>\n",
       "      <td>0.267508</td>\n",
       "      <td>3.889462</td>\n",
       "      <td>1.807334</td>\n",
       "      <td>0.801858</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.321959</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.204855</td>\n",
       "      <td>1.850370</td>\n",
       "      <td>1.943691</td>\n",
       "      <td>0.577681</td>\n",
       "      <td>0.119134</td>\n",
       "      <td>2.543613</td>\n",
       "      <td>1.563417</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.741906</td>\n",
       "      <td>0.315360</td>\n",
       "      <td>2.057147</td>\n",
       "      <td>5.647437</td>\n",
       "      <td>2.449878</td>\n",
       "      <td>1.865399</td>\n",
       "      <td>7.699445</td>\n",
       "      <td>1.951591</td>\n",
       "      <td>3.003493</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.950126</td>\n",
       "      <td>7.994647</td>\n",
       "      <td>3.232115</td>\n",
       "      <td>3.141074</td>\n",
       "      <td>3.768372</td>\n",
       "      <td>2.212054</td>\n",
       "      <td>6.470033</td>\n",
       "      <td>0.429892</td>\n",
       "      <td>1.882017</td>\n",
       "      <td>3.695462</td>\n",
       "      <td>...</td>\n",
       "      <td>1.492471</td>\n",
       "      <td>3.070536</td>\n",
       "      <td>3.717540</td>\n",
       "      <td>4.355633</td>\n",
       "      <td>2.968318</td>\n",
       "      <td>5.436883</td>\n",
       "      <td>1.981544</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.376758</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.636942</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.152899</td>\n",
       "      <td>2.104569</td>\n",
       "      <td>1.427852</td>\n",
       "      <td>2.474058</td>\n",
       "      <td>1.626031</td>\n",
       "      <td>0.458366</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.750136</td>\n",
       "      <td>...</td>\n",
       "      <td>2.700227</td>\n",
       "      <td>1.449174</td>\n",
       "      <td>1.987443</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.406036</td>\n",
       "      <td>4.275665</td>\n",
       "      <td>0.717273</td>\n",
       "      <td>5.197172</td>\n",
       "      <td>2.265270</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54995</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.545820</td>\n",
       "      <td>5.057974</td>\n",
       "      <td>4.011364</td>\n",
       "      <td>2.313361</td>\n",
       "      <td>3.901836</td>\n",
       "      <td>0.573626</td>\n",
       "      <td>3.748518</td>\n",
       "      <td>3.378146</td>\n",
       "      <td>...</td>\n",
       "      <td>2.491682</td>\n",
       "      <td>7.055640</td>\n",
       "      <td>4.191986</td>\n",
       "      <td>3.419419</td>\n",
       "      <td>1.887239</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.546530</td>\n",
       "      <td>2.499541</td>\n",
       "      <td>1.345368</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54996</th>\n",
       "      <td>0.338265</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.886091</td>\n",
       "      <td>3.754637</td>\n",
       "      <td>5.158128</td>\n",
       "      <td>2.076467</td>\n",
       "      <td>2.599750</td>\n",
       "      <td>0.556816</td>\n",
       "      <td>3.748872</td>\n",
       "      <td>2.478272</td>\n",
       "      <td>...</td>\n",
       "      <td>6.908456</td>\n",
       "      <td>3.629078</td>\n",
       "      <td>1.206927</td>\n",
       "      <td>0.725747</td>\n",
       "      <td>3.246931</td>\n",
       "      <td>0.209125</td>\n",
       "      <td>6.530942</td>\n",
       "      <td>0.927235</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54997</th>\n",
       "      <td>2.203880</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.390905</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.760312</td>\n",
       "      <td>2.231620</td>\n",
       "      <td>0.722963</td>\n",
       "      <td>0.847233</td>\n",
       "      <td>1.047008</td>\n",
       "      <td>4.111372</td>\n",
       "      <td>...</td>\n",
       "      <td>2.429658</td>\n",
       "      <td>3.717557</td>\n",
       "      <td>2.442240</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.157865</td>\n",
       "      <td>6.601553</td>\n",
       "      <td>0.526058</td>\n",
       "      <td>1.912827</td>\n",
       "      <td>1.567822</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54998</th>\n",
       "      <td>3.899356</td>\n",
       "      <td>6.943176</td>\n",
       "      <td>2.287122</td>\n",
       "      <td>1.937744</td>\n",
       "      <td>1.183595</td>\n",
       "      <td>6.381529</td>\n",
       "      <td>5.689101</td>\n",
       "      <td>2.186828</td>\n",
       "      <td>1.163785</td>\n",
       "      <td>1.405378</td>\n",
       "      <td>...</td>\n",
       "      <td>3.054497</td>\n",
       "      <td>1.990320</td>\n",
       "      <td>1.662855</td>\n",
       "      <td>3.331782</td>\n",
       "      <td>1.160073</td>\n",
       "      <td>9.504177</td>\n",
       "      <td>2.178482</td>\n",
       "      <td>0.105665</td>\n",
       "      <td>3.549601</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54999</th>\n",
       "      <td>0.952590</td>\n",
       "      <td>1.276391</td>\n",
       "      <td>0.997754</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.813088</td>\n",
       "      <td>4.919580</td>\n",
       "      <td>5.429385</td>\n",
       "      <td>3.239059</td>\n",
       "      <td>2.902877</td>\n",
       "      <td>2.859775</td>\n",
       "      <td>...</td>\n",
       "      <td>2.382362</td>\n",
       "      <td>4.064071</td>\n",
       "      <td>2.368603</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.141384</td>\n",
       "      <td>3.135317</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.501769</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>55000 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6   \\\n",
       "0      5.398752  6.278047  3.580938  0.000000  0.000000  1.484455  6.600603   \n",
       "1      1.725769  0.000000  3.281141  3.571757  3.438337  2.027230  2.803521   \n",
       "2      5.321959  0.000000  3.204855  1.850370  1.943691  0.577681  0.119134   \n",
       "3      2.950126  7.994647  3.232115  3.141074  3.768372  2.212054  6.470033   \n",
       "4      1.636942  0.000000  1.152899  2.104569  1.427852  2.474058  1.626031   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "54995  0.000000  0.000000  1.545820  5.057974  4.011364  2.313361  3.901836   \n",
       "54996  0.338265  0.000000  1.886091  3.754637  5.158128  2.076467  2.599750   \n",
       "54997  2.203880  0.000000  3.390905  0.000000  5.760312  2.231620  0.722963   \n",
       "54998  3.899356  6.943176  2.287122  1.937744  1.183595  6.381529  5.689101   \n",
       "54999  0.952590  1.276391  0.997754  0.000000  1.813088  4.919580  5.429385   \n",
       "\n",
       "             7         8         9   ...        90        91        92  \\\n",
       "0      1.099844  0.000000  0.000000  ...  2.563326  2.124260  5.068234   \n",
       "1      0.249094  2.896202  1.898718  ...  6.233778  3.301990  2.414010   \n",
       "2      2.543613  1.563417  0.000000  ...  4.741906  0.315360  2.057147   \n",
       "3      0.429892  1.882017  3.695462  ...  1.492471  3.070536  3.717540   \n",
       "4      0.458366  0.000000  1.750136  ...  2.700227  1.449174  1.987443   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "54995  0.573626  3.748518  3.378146  ...  2.491682  7.055640  4.191986   \n",
       "54996  0.556816  3.748872  2.478272  ...  6.908456  3.629078  1.206927   \n",
       "54997  0.847233  1.047008  4.111372  ...  2.429658  3.717557  2.442240   \n",
       "54998  2.186828  1.163785  1.405378  ...  3.054497  1.990320  1.662855   \n",
       "54999  3.239059  2.902877  2.859775  ...  2.382362  4.064071  2.368603   \n",
       "\n",
       "             93        94        95        96        97        98   99  \n",
       "0      5.630467  5.340586  0.000000  1.196835  0.000000  5.042018  0.0  \n",
       "1      0.000000  2.431331  0.267508  3.889462  1.807334  0.801858  0.0  \n",
       "2      5.647437  2.449878  1.865399  7.699445  1.951591  3.003493  0.0  \n",
       "3      4.355633  2.968318  5.436883  1.981544  0.000000  1.376758  0.0  \n",
       "4      0.000000  3.406036  4.275665  0.717273  5.197172  2.265270  0.0  \n",
       "...         ...       ...       ...       ...       ...       ...  ...  \n",
       "54995  3.419419  1.887239  0.000000  2.546530  2.499541  1.345368  0.0  \n",
       "54996  0.725747  3.246931  0.209125  6.530942  0.927235  0.000000  0.0  \n",
       "54997  0.000000  1.157865  6.601553  0.526058  1.912827  1.567822  0.0  \n",
       "54998  3.331782  1.160073  9.504177  2.178482  0.105665  3.549601  0.0  \n",
       "54999  0.000000  0.000000  5.141384  3.135317  0.000000  0.501769  0.0  \n",
       "\n",
       "[55000 rows x 100 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.398752</td>\n",
       "      <td>6.278047</td>\n",
       "      <td>3.580938</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.484455</td>\n",
       "      <td>6.600603</td>\n",
       "      <td>1.099844</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.563326</td>\n",
       "      <td>2.124260</td>\n",
       "      <td>5.068234</td>\n",
       "      <td>5.630467</td>\n",
       "      <td>5.340586</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.196835</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.042018</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.725769</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.281141</td>\n",
       "      <td>3.571757</td>\n",
       "      <td>3.438337</td>\n",
       "      <td>2.027230</td>\n",
       "      <td>2.803521</td>\n",
       "      <td>0.249094</td>\n",
       "      <td>2.896202</td>\n",
       "      <td>1.898718</td>\n",
       "      <td>...</td>\n",
       "      <td>6.233778</td>\n",
       "      <td>3.301990</td>\n",
       "      <td>2.414010</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.431331</td>\n",
       "      <td>0.267508</td>\n",
       "      <td>3.889462</td>\n",
       "      <td>1.807334</td>\n",
       "      <td>0.801858</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.321959</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.204855</td>\n",
       "      <td>1.850370</td>\n",
       "      <td>1.943691</td>\n",
       "      <td>0.577681</td>\n",
       "      <td>0.119134</td>\n",
       "      <td>2.543613</td>\n",
       "      <td>1.563417</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.741906</td>\n",
       "      <td>0.315360</td>\n",
       "      <td>2.057147</td>\n",
       "      <td>5.647437</td>\n",
       "      <td>2.449878</td>\n",
       "      <td>1.865399</td>\n",
       "      <td>7.699445</td>\n",
       "      <td>1.951591</td>\n",
       "      <td>3.003493</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.950126</td>\n",
       "      <td>7.994647</td>\n",
       "      <td>3.232115</td>\n",
       "      <td>3.141074</td>\n",
       "      <td>3.768372</td>\n",
       "      <td>2.212054</td>\n",
       "      <td>6.470033</td>\n",
       "      <td>0.429892</td>\n",
       "      <td>1.882017</td>\n",
       "      <td>3.695462</td>\n",
       "      <td>...</td>\n",
       "      <td>1.492471</td>\n",
       "      <td>3.070536</td>\n",
       "      <td>3.717540</td>\n",
       "      <td>4.355633</td>\n",
       "      <td>2.968318</td>\n",
       "      <td>5.436883</td>\n",
       "      <td>1.981544</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.376758</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.636942</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.152899</td>\n",
       "      <td>2.104569</td>\n",
       "      <td>1.427852</td>\n",
       "      <td>2.474058</td>\n",
       "      <td>1.626031</td>\n",
       "      <td>0.458366</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.750136</td>\n",
       "      <td>...</td>\n",
       "      <td>2.700227</td>\n",
       "      <td>1.449174</td>\n",
       "      <td>1.987443</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.406036</td>\n",
       "      <td>4.275665</td>\n",
       "      <td>0.717273</td>\n",
       "      <td>5.197172</td>\n",
       "      <td>2.265270</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54995</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.545820</td>\n",
       "      <td>5.057974</td>\n",
       "      <td>4.011364</td>\n",
       "      <td>2.313361</td>\n",
       "      <td>3.901836</td>\n",
       "      <td>0.573626</td>\n",
       "      <td>3.748518</td>\n",
       "      <td>3.378146</td>\n",
       "      <td>...</td>\n",
       "      <td>2.491682</td>\n",
       "      <td>7.055640</td>\n",
       "      <td>4.191986</td>\n",
       "      <td>3.419419</td>\n",
       "      <td>1.887239</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.546530</td>\n",
       "      <td>2.499541</td>\n",
       "      <td>1.345368</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54996</th>\n",
       "      <td>0.338265</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.886091</td>\n",
       "      <td>3.754637</td>\n",
       "      <td>5.158128</td>\n",
       "      <td>2.076467</td>\n",
       "      <td>2.599750</td>\n",
       "      <td>0.556816</td>\n",
       "      <td>3.748872</td>\n",
       "      <td>2.478272</td>\n",
       "      <td>...</td>\n",
       "      <td>6.908456</td>\n",
       "      <td>3.629078</td>\n",
       "      <td>1.206927</td>\n",
       "      <td>0.725747</td>\n",
       "      <td>3.246931</td>\n",
       "      <td>0.209125</td>\n",
       "      <td>6.530942</td>\n",
       "      <td>0.927235</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54997</th>\n",
       "      <td>2.203880</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.390905</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.760312</td>\n",
       "      <td>2.231620</td>\n",
       "      <td>0.722963</td>\n",
       "      <td>0.847233</td>\n",
       "      <td>1.047008</td>\n",
       "      <td>4.111372</td>\n",
       "      <td>...</td>\n",
       "      <td>2.429658</td>\n",
       "      <td>3.717557</td>\n",
       "      <td>2.442240</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.157865</td>\n",
       "      <td>6.601553</td>\n",
       "      <td>0.526058</td>\n",
       "      <td>1.912827</td>\n",
       "      <td>1.567822</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54998</th>\n",
       "      <td>3.899356</td>\n",
       "      <td>6.943176</td>\n",
       "      <td>2.287122</td>\n",
       "      <td>1.937744</td>\n",
       "      <td>1.183595</td>\n",
       "      <td>6.381529</td>\n",
       "      <td>5.689101</td>\n",
       "      <td>2.186828</td>\n",
       "      <td>1.163785</td>\n",
       "      <td>1.405378</td>\n",
       "      <td>...</td>\n",
       "      <td>3.054497</td>\n",
       "      <td>1.990320</td>\n",
       "      <td>1.662855</td>\n",
       "      <td>3.331782</td>\n",
       "      <td>1.160073</td>\n",
       "      <td>9.504177</td>\n",
       "      <td>2.178482</td>\n",
       "      <td>0.105665</td>\n",
       "      <td>3.549601</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54999</th>\n",
       "      <td>0.952590</td>\n",
       "      <td>1.276391</td>\n",
       "      <td>0.997754</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.813088</td>\n",
       "      <td>4.919580</td>\n",
       "      <td>5.429385</td>\n",
       "      <td>3.239059</td>\n",
       "      <td>2.902877</td>\n",
       "      <td>2.859775</td>\n",
       "      <td>...</td>\n",
       "      <td>2.382362</td>\n",
       "      <td>4.064071</td>\n",
       "      <td>2.368603</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.141384</td>\n",
       "      <td>3.135317</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.501769</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>55000 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6   \\\n",
       "0      5.398752  6.278047  3.580938  0.000000  0.000000  1.484455  6.600603   \n",
       "1      1.725769  0.000000  3.281141  3.571757  3.438337  2.027230  2.803521   \n",
       "2      5.321959  0.000000  3.204855  1.850370  1.943691  0.577681  0.119134   \n",
       "3      2.950126  7.994647  3.232115  3.141074  3.768372  2.212054  6.470033   \n",
       "4      1.636942  0.000000  1.152899  2.104569  1.427852  2.474058  1.626031   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "54995  0.000000  0.000000  1.545820  5.057974  4.011364  2.313361  3.901836   \n",
       "54996  0.338265  0.000000  1.886091  3.754637  5.158128  2.076467  2.599750   \n",
       "54997  2.203880  0.000000  3.390905  0.000000  5.760312  2.231620  0.722963   \n",
       "54998  3.899356  6.943176  2.287122  1.937744  1.183595  6.381529  5.689101   \n",
       "54999  0.952590  1.276391  0.997754  0.000000  1.813088  4.919580  5.429385   \n",
       "\n",
       "             7         8         9   ...        90        91        92  \\\n",
       "0      1.099844  0.000000  0.000000  ...  2.563326  2.124260  5.068234   \n",
       "1      0.249094  2.896202  1.898718  ...  6.233778  3.301990  2.414010   \n",
       "2      2.543613  1.563417  0.000000  ...  4.741906  0.315360  2.057147   \n",
       "3      0.429892  1.882017  3.695462  ...  1.492471  3.070536  3.717540   \n",
       "4      0.458366  0.000000  1.750136  ...  2.700227  1.449174  1.987443   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "54995  0.573626  3.748518  3.378146  ...  2.491682  7.055640  4.191986   \n",
       "54996  0.556816  3.748872  2.478272  ...  6.908456  3.629078  1.206927   \n",
       "54997  0.847233  1.047008  4.111372  ...  2.429658  3.717557  2.442240   \n",
       "54998  2.186828  1.163785  1.405378  ...  3.054497  1.990320  1.662855   \n",
       "54999  3.239059  2.902877  2.859775  ...  2.382362  4.064071  2.368603   \n",
       "\n",
       "             93        94        95        96        97        98   99  \n",
       "0      5.630467  5.340586  0.000000  1.196835  0.000000  5.042018  0.0  \n",
       "1      0.000000  2.431331  0.267508  3.889462  1.807334  0.801858  0.0  \n",
       "2      5.647437  2.449878  1.865399  7.699445  1.951591  3.003493  0.0  \n",
       "3      4.355633  2.968318  5.436883  1.981544  0.000000  1.376758  0.0  \n",
       "4      0.000000  3.406036  4.275665  0.717273  5.197172  2.265270  0.0  \n",
       "...         ...       ...       ...       ...       ...       ...  ...  \n",
       "54995  3.419419  1.887239  0.000000  2.546530  2.499541  1.345368  0.0  \n",
       "54996  0.725747  3.246931  0.209125  6.530942  0.927235  0.000000  0.0  \n",
       "54997  0.000000  1.157865  6.601553  0.526058  1.912827  1.567822  0.0  \n",
       "54998  3.331782  1.160073  9.504177  2.178482  0.105665  3.549601  0.0  \n",
       "54999  0.000000  0.000000  5.141384  3.135317  0.000000  0.501769  0.0  \n",
       "\n",
       "[55000 rows x 100 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[1600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label列名可以自定义，现在是 label_ + 数字 的格式\n",
    "Y_label = pd.DataFrame(Y,columns=['label_'+ str(x) for x in range(len(Y[0]))])\n",
    "C_label = pd.DataFrame(C,columns=['label_'+ str(x) for x in range(len(Y[0]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_train_data_withlable=pd.concat([encoded_train_data,Y_label],axis=1)\n",
    "encoded_test_data_withlable=pd.concat([encoded_test_data,C_label],axis=1)\n",
    "\n",
    "\n",
    "#encoded_train_data.insert(weidu_encoded,str(weidu_encoded),y_train)#(第几列后面插入，列名，列数据)\n",
    "#encoded_test_data.insert(weidu_encoded,str(weidu_encoded),y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#列名转为字符串\n",
    "encoded_train_data_withlable.columns.columns=encoded_train_data_withlable.columns.map(str)\n",
    "encoded_test_data_withlable.columns.columns=encoded_test_data_withlable.columns.map(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存编码结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_train_data_withlable.to_csv(r'encoded_train_data.csv')\n",
    "encoded_test_data_withlable.to_csv(r'encoded_test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IDS-1",
   "language": "python",
   "name": "ids-1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
